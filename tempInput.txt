Figure 5B shows the average recovery accuracy at coverage 20–50. Given a specific coverage, the recovery accuracy decreases as the error rate increases. However, it increases as the coverage increases at a specific error rate. As the error rate gets as low as 0.01–0.02, the proposed methods can recover even more than 99% errors at coverage 20. When the error rate is about 0.05, the proposed method can still recover more 98% errors at coverage 50. As the error rate is very high at 0.1, the recovery accuracy drops dramatically to about 50–60%. The main reason may be that our code book can only correct most of the one-base error. When the error is more than one, we only substitute it with a possible legal code so that the current insertions or deletions may not influence the subsequent codes. Therefore, the error degree at 0.1 may be beyond the code capability in the first-level correction.

Figure 5C further shows the average proportion of the right corrections accomplished by the three levels. Obviously, the first-level correction plays a dominate role and the amount of errors corrected by it increases as the error rate increases. The error correction ability of Table 1 determines the final performance of the proposed method. As the error rate increases, the deterioration of the code corrections will reduce the correction ability of the multiple alignments in the second level. Then, the proportion of the second level gradually decreases with the error rate increasing. Finally, the spelling check in the third level always helps to correct a small portion of the errors in word level.

Figure 5D shows average decoding time of the proposed method at coverage 20–50. The time need to recover the original files is linearly proportional to the coverage or error rates. In our experiment, the first-level decoding time is about 10 min and the third level takes only about 1 min. Clustering the text lines and purifying them in the second level cost most of the processing time.

The Analysis of the Code Robustness
The simple error case is that there are only substitution errors. If a code undergoes more than 1 random substitutions, it is hard to determine which legal code it comes from as the minimal Hamming distance between codes is 3. Given the substitution error as 2%, the probability of a legal code c mutated with at least 2 bases is about 0.6%. This means the probability that a code cannot be correctly distinguished is about 0.6% under substitution errors. However, when the substitution error is 10%, the probability of a legal code c mutated with at least 2 bases will increase to about 11%. Considering the complexity of insertion and deletion, this is the main reason leading to the drastic decline of accuracy.
On the other hand, a code may be mutated to other code or more similar with that code instead of itself. For example, the Hamming distance between codes ‘ATCACG’ and ‘ATGAGC’ is 3. If the last two base ‘CG’ in ‘ATCACG’ is mutated into ‘CG’, then it will be corrected with the second code ‘ATGAGC’. If it is assumed that a base is equally mutated with probability 25%, then ‘ATCACG’ may be treated as ‘ATGAGC’ by mistake with probability 1.4%.

Considering the composite errors with substitution, insertion and deletion, it is possible that one code may be mutated to other codes through less composite mutations. For example, code ‘TAAGGC’ may become ‘TAGGCT’ with a deletion of ‘A’ and an insertion of ‘T’. If a deletion is determined to happen in ‘TAGGC’, then there are two selections. One is a deletion ‘A’ in the middle while the other is a deletion ‘T’ in t